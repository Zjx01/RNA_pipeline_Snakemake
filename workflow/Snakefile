import glob
import os
from snakemake.utils import min_version
import yaml 

##### set minimum snakemake version #####
min_version("8.8.0")



#### setup report ####
#### configfile:"config/config.yaml"####

with open('/Users/zhaoj11/Documents/RNA_pipeline_Snakemake/config/config.yaml', 'r') as config_file:
    config = yaml.safe_load(config_file)


with open(config['sample_file']) as fh:
    sample_reads = dict()
    for line in fh:
        fields = line.strip().split('\t')
        files = [
            path
            for f in fields[1:]
            for path in glob.glob(f)
        ]
        if len(files) != 2:
            raise ValueError(f"Expected two read files per samples, {fields[0]} has {len(files)}: {files}")
        sample_reads[fields[0]] = files
config['sample_reads'] = sample_reads


def get_input_fastq(wildcards):
    return config['sample_reads'][wildcards.sample]
    
rule all:
    input:expand("results/fastqc/{sample}",sample=config["sample_reads"]),
        expand("results/trimmed/{sample}/{sample}_R1.fastq.gz", sample=config["sample_reads"]),
        expand("results/trimmed/{sample}/{sample}_R2.fastq.gz", sample=config["sample_reads"]),
        expand("results/trimmed/{sample}/{sample}_merged.fastq.gz", sample=config["sample_reads"]),
        "resources/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz"

	
rule fastqc:
    input: get_input_fastq
    output: directory("results/fastqc/{sample}/") 
    log:
    	"logs/fastqc/{sample}.log"
    threads:1
    resources:
        mem_mb = 20000
    conda: "envs/fastqc.yaml"
    shell:
        "mkdir -p results/fastqc/{wildcards.sample} && fastqc -o {output} -t {threads} {input}"



rule trim:
    input:
        "/Users/zhaoj11/Documents/BinPipeline/fastq/{sample}_R1.fastq.gz",
        "/Users/zhaoj11/Documents/BinPipeline/fastq/{sample}_R2.fastq.gz"
    output:
        r1="results/trimmed/{sample}/{sample}_R1.fastq.gz",
        r2="results/trimmed/{sample}/{sample}_R2.fastq.gz",
        merged="results/trimmed/{sample}/{sample}_merged.fastq.gz"
    log:
        "logs/fastp-trim/{sample}.log"
    threads: 1
    conda:
        "envs/fastp.yaml"
    shell:
        """
        fastp \
            --in1 {input[0]} \
            --in2 {input[1]} \
            --out1 {output.r1} \
            --out2 {output.r2} \
            -q 20 -m --merged_out {output.merged} \
            > {log} 2>&1
        """



rule human_filter_reference:
    params: 
        url=config["ref_ftp"]
    output:
        "resources/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz"
    conda:
        "envs/wget.yaml"	
    shell:
        "wget --no-check-certificate -O {output} {params.url}"


rule human_filter_bowtie2_index:
    input: rules.human_filter_reference.output
    output:
        multiext(
            "resources/Homo_sapiens.GRCh38.dna.primary_assembly",
            ".1.bt2",
            ".2.bt2",
            ".3.bt2",
            ".4.bt2",
            ".rev.1.bt2",
            ".rev.2.bt2"
        )
    log: "logs/human_filter/bowtie2/index.log"
    threads: 30
    resources: mem_mb=lambda wildcards, input, attempt: max(input.size_mb * 4 * attempt, 40960)
    conda: "envs/bowtie2.yaml"
    shell:
        "bowtie2-build"
        " --threads {threads}"
        " {input}"
        " resources/Homo_sapiens.GRCh38.dna.primary_assembly"
        " &> {log}"


rule human_genome_filter:
    input:
        index=rules.human_filter_bowtie2_index.output,
        merged_fq=rules.trim.output[2]
    output:
        metagenome="results/metagenome/{sample}/{sample}.fastq"
    log: "logs/human_filter/bowtie2/humangenome_filter/{sample}.log"
    conda: "envs/bowtie2.yaml"
    threads: 20
    # resources: mem_mb=lambda wildcards, input, attempt: max(input.size_mb * 4 * attempt, 40960)
    shell:
        "bowtie2"
        " --threads {threads}"
        " -x resources/Homo_sapiens.GRCh38.dna.primary_assembly"
        " -U {merged_fq}"
        " -un {metagenome}"
        " &> {log}" 


